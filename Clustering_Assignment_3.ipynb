{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Assignment - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Explain the basic concept of clustering and give examples of applications where clustering is useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Clustering is a fundamental technique in unsupervised learning where the goal is to partition a dataset into groups, or clusters, of data points that are similar to each other within the same cluster and dissimilar to data points in other clusters. The basic concept involves grouping data points based on their similarity or distance in a feature space, without any prior knowledge of the underlying class labels.\n",
    "\n",
    "Here's an explanation of the basic concept of clustering:\n",
    "\n",
    "Grouping Similar Data: Clustering algorithms aim to identify natural groupings or patterns in the data based on similarities between data points. Similarity is typically measured using a distance metric, where data points that are closer together in the feature space are considered more similar.\n",
    "\n",
    "Unsupervised Learning: Unlike supervised learning, clustering does not require labeled data. Instead, it automatically discovers the structure of the data without any predefined categories or class labels.\n",
    "\n",
    "Partitioning Data: Clustering algorithms partition the dataset into clusters such that data points within the same cluster are more similar to each other than they are to data points in other clusters. The number of clusters may be predefined by the user or determined automatically by the algorithm.\n",
    "\n",
    "Exploratory Analysis: Clustering is often used for exploratory data analysis to uncover hidden patterns, groupings, or insights in the data. It can reveal underlying structures in the data that may not be apparent from the raw data alone.\n",
    "\n",
    "Examples of applications where clustering is useful include:\n",
    "\n",
    "Customer Segmentation: Clustering can be used to segment customers based on their purchasing behavior, demographics, or preferences. This information can then be used for targeted marketing campaigns or personalized recommendations.\n",
    "\n",
    "Image Segmentation: In computer vision, clustering is used for image segmentation, where pixels with similar characteristics (such as color or texture) are grouped together to identify objects or regions of interest in images.\n",
    "\n",
    "Anomaly Detection: Clustering can be used to detect anomalies or outliers in datasets by identifying data points that do not fit well into any of the clusters. This is useful for fraud detection, network intrusion detection, or identifying abnormal behavior in systems.\n",
    "\n",
    "Document Clustering: Clustering can be applied to text data for document clustering or topic modeling. Documents with similar content or themes can be grouped together, enabling tasks such as document organization, summarization, or information retrieval.\n",
    "\n",
    "Genomic Clustering: In bioinformatics, clustering is used to analyze gene expression data or DNA sequences to identify patterns or relationships between genes. This can help in understanding gene functions, disease mechanisms, or evolutionary relationships.\n",
    "\n",
    "These are just a few examples of the many applications where clustering is useful. In general, clustering is a versatile technique that finds applications across various domains, including marketing, healthcare, finance, and many others.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What is DBSCAN and how does it differ from other clustering algorithms such as k-means and\n",
    "hierarchical clustering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm used for identifying clusters of varying shapes and sizes in a dataset. Unlike k-means and hierarchical clustering, which are centroid-based and hierarchical-based clustering algorithms respectively, DBSCAN operates based on the density of data points in the feature space. Here's how DBSCAN differs from k-means and hierarchical clustering:\n",
    "\n",
    "Density-Based Approach:\n",
    "\n",
    "DBSCAN defines clusters as dense regions of data points separated by regions of lower density.\n",
    "It doesn't require a predefined number of clusters and can automatically detect clusters of arbitrary shapes and sizes.\n",
    "Points that are within a dense neighborhood of a core point are assigned to the same cluster, while points that are in low-density regions or outliers are labeled as noise.\n",
    "No Centroid Calculation:\n",
    "\n",
    "Unlike k-means, which requires calculating the centroids of clusters, DBSCAN does not compute centroids.\n",
    "DBSCAN operates by directly identifying dense regions in the data space based on a specified distance threshold (\n",
    "�\n",
    "ε) and minimum number of points (\n",
    "minPts\n",
    "minPts) required to form a cluster.\n",
    "Handling Noise and Outliers:\n",
    "\n",
    "DBSCAN is robust to noise and outliers because it explicitly identifies them as points that do not belong to any cluster.\n",
    "Noise points are labeled as outliers, allowing for the detection of anomalies in the data.\n",
    "Ability to Detect Arbitrary-Shaped Clusters:\n",
    "\n",
    "DBSCAN is capable of identifying clusters of arbitrary shapes and sizes, including clusters that are non-linear or irregularly shaped.\n",
    "This makes DBSCAN particularly suitable for datasets with complex structures or overlapping clusters.\n",
    "Parameter Sensitivity:\n",
    "\n",
    "DBSCAN requires setting two parameters: \n",
    "�\n",
    "ε (the maximum distance between two points to be considered neighbors) and \n",
    "minPts\n",
    "minPts (the minimum number of points required to form a dense region or cluster).\n",
    "The choice of these parameters can significantly affect the clustering results, and selecting appropriate values requires careful consideration of the dataset characteristics.\n",
    "In summary, DBSCAN offers several advantages over k-means and hierarchical clustering, including its ability to handle noise and outliers, detect clusters of arbitrary shapes, and operate without the need for predefined cluster centroids. However, it requires careful parameter tuning and may be less efficient for very large datasets compared to some other clustering algorithms.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. How do you determine the optimal values for the epsilon and minimum points parameters in DBSCAN\n",
    "clustering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Determining the optimal values for the epsilon (\n",
    "�\n",
    "ε) and minimum points (\n",
    "minPts\n",
    "minPts) parameters in DBSCAN clustering is crucial for obtaining meaningful clustering results. Here are several approaches commonly used to determine these parameters:\n",
    "\n",
    "Visual Inspection:\n",
    "\n",
    "Visualize the data and plot the distribution of distances between points (e.g., using a histogram or scatter plot).\n",
    "Identify a reasonable range of distances that capture the local density of the data. This can help in choosing an appropriate value for \n",
    "�\n",
    "ε.\n",
    "K-Distance Plot:\n",
    "\n",
    "Compute the distance from each point to its \n",
    "�\n",
    "k-th nearest neighbor, where \n",
    "�\n",
    "k is typically set to a small value (e.g., \n",
    "�\n",
    "=\n",
    "minPts\n",
    "k=minPts).\n",
    "Plot these distances in ascending order to create a k-distance plot.\n",
    "Observe the plot to identify a \"knee\" or \"elbow\" point, which indicates a significant change in the rate of distance increase. This knee point can provide a suitable value for \n",
    "�\n",
    "ε.\n",
    "Reachability Plot:\n",
    "\n",
    "Compute the reachability distance for each point, which measures the distance at which a point can be reached from its nearest core point.\n",
    "Plot the reachability distances in ascending order.\n",
    "Look for a significant increase in reachability distance, which may indicate the presence of clusters and can help in choosing \n",
    "�\n",
    "ε.\n",
    "Silhouette Score:\n",
    "\n",
    "Use the silhouette score to evaluate the quality of clustering for different combinations of \n",
    "�\n",
    "ε and \n",
    "minPts\n",
    "minPts.\n",
    "Compute the silhouette score for each clustering result and select the combination of parameters that maximizes the silhouette score.\n",
    "This approach provides a quantitative measure of clustering quality, taking into account both cohesion and separation of clusters.\n",
    "Grid Search:\n",
    "\n",
    "Perform a grid search over a range of values for \n",
    "�\n",
    "ε and \n",
    "minPts\n",
    "minPts.\n",
    "Evaluate the clustering performance using a validation metric such as silhouette score, Davies-Bouldin index, or another suitable criterion.\n",
    "Choose the combination of parameters that yields the best clustering performance on the validation set.\n",
    "Domain Knowledge:\n",
    "\n",
    "Consider domain-specific knowledge or insights about the data when selecting values for \n",
    "�\n",
    "ε and \n",
    "minPts\n",
    "minPts.\n",
    "For example, if you know that the data has certain characteristics or natural scales, you can use this information to inform the choice of parameters.\n",
    "It's important to note that there is no universally optimal set of parameters for DBSCAN, as the \"optimal\" values depend on the specific characteristics of the dataset and the clustering task at hand. Therefore, it's often necessary to experiment with different parameter values and evaluate their impact on clustering performance using one or more of the techniques mentioned above.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. How does DBSCAN clustering handle outliers in a dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a clustering algorithm that is particularly effective at handling outliers in a dataset. Here's how it works:\n",
    "\n",
    "Core Points: DBSCAN defines two parameters: epsilon (ε) and MinPts. For each point in the dataset, it counts how many points are within a distance of ε. If the number of points (including the point itself) is greater than or equal to MinPts, the point is considered a core point.\n",
    "\n",
    "Border Points: Border points are points that are not core points but are within ε distance of at least one core point.\n",
    "\n",
    "Noise Points (Outliers): Points that are neither core points nor border points are considered noise points or outliers.\n",
    "\n",
    "Cluster Formation: DBSCAN forms clusters by connecting core points to their neighboring core points (directly or indirectly). Core points that are reachable from each other form a single cluster. Border points may be included in a cluster if they are reachable from a core point.\n",
    "\n",
    "Outlier Detection: Points that are not assigned to any cluster are considered outliers. These are the noise points mentioned earlier.\n",
    "\n",
    "In summary, DBSCAN handles outliers by labeling them as noise points if they are not close enough to a sufficient number of other points (core points) to form a cluster. This makes DBSCAN robust to outliers and capable of discovering clusters of arbitrary shapes and sizes.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. How does DBSCAN clustering differ from k-means clustering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) and k-means clustering are both popular clustering algorithms, but they differ in several key aspects:\n",
    "\n",
    "Clustering Approach:\n",
    "\n",
    "DBSCAN: It is a density-based clustering algorithm that groups together closely packed points based on a density criterion. It doesn't require the number of clusters to be specified beforehand and can identify clusters of arbitrary shapes and sizes.\n",
    "K-means: It is a centroid-based clustering algorithm that partitions the dataset into a pre-specified number of clusters (k) by minimizing the sum of squared distances between data points and the centroids of their respective clusters.\n",
    "Cluster Shape:\n",
    "\n",
    "DBSCAN: It can find clusters of arbitrary shapes and sizes since it relies on the density of points.\n",
    "K-means: It assumes that clusters are spherical and isotropic (having the same shape in all directions) and may not perform well with clusters of irregular shapes or sizes.\n",
    "Handling Outliers:\n",
    "\n",
    "DBSCAN: It can handle outliers effectively by classifying them as noise points if they don't belong to any dense region. Outliers are not assigned to any cluster.\n",
    "K-means: It treats outliers as part of the closest cluster, which may lead to inaccurate cluster assignments if there are many outliers.\n",
    "Parameter Dependency:\n",
    "\n",
    "DBSCAN: It requires tuning two parameters: epsilon (ε), the maximum distance between two points to be considered neighbors, and MinPts, the minimum number of points within ε to form a dense region. However, DBSCAN is less sensitive to the choice of these parameters than k-means is to the initial choice of centroids.\n",
    "K-means: It requires specifying the number of clusters (k) beforehand, which can be challenging, especially when the number of clusters is not known a priori. Additionally, k-means is sensitive to the initial placement of centroids, which can lead to different results with different initializations.\n",
    "Scalability:\n",
    "\n",
    "DBSCAN: It can be more computationally expensive, especially for large datasets, as it needs to compute pairwise distances between points.\n",
    "K-means: It is generally faster and more scalable, making it suitable for large datasets, particularly when using optimized implementations.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. Can DBSCAN clustering be applied to datasets with high dimensional feature spaces? If so, what are\n",
    "some potential challenges?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Yes, DBSCAN clustering can be applied to datasets with high-dimensional feature spaces. However, there are some potential challenges associated with applying DBSCAN to high-dimensional data:\n",
    "\n",
    "Curse of Dimensionality: In high-dimensional spaces, the density of data points tends to decrease as the number of dimensions increases. This phenomenon, known as the curse of dimensionality, can make it challenging to define a suitable distance metric or determine appropriate values for the epsilon (ε) and MinPts parameters in DBSCAN.\n",
    "\n",
    "Sparse Data: High-dimensional spaces often lead to sparse data, where the majority of data points are far apart from each other. DBSCAN relies on the notion of density, and in sparse spaces, it may be difficult to identify dense regions, leading to suboptimal clustering results.\n",
    "\n",
    "Increased Computational Complexity: As the dimensionality of the data increases, the computational complexity of DBSCAN also increases. DBSCAN computes pairwise distances between points, and in high-dimensional spaces, the number of distances to compute grows rapidly, leading to higher computational costs.\n",
    "\n",
    "Dimensionality Reduction: In many cases, dimensionality reduction techniques such as PCA (Principal Component Analysis) or t-SNE (t-Distributed Stochastic Neighbor Embedding) are applied before clustering high-dimensional data. These techniques can help mitigate the curse of dimensionality and improve the performance of DBSCAN by reducing the dimensionality of the data while preserving its structure.\n",
    "\n",
    "Parameter Tuning: Selecting appropriate values for the epsilon (ε) and MinPts parameters becomes more challenging in high-dimensional spaces. It may require more extensive experimentation or domain knowledge to determine suitable parameter values that yield meaningful clustering results.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. How does DBSCAN clustering handle clusters with varying densities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is well-suited for handling clusters with varying densities. Here's how it achieves this:\n",
    "\n",
    "Density-based Criterion: DBSCAN defines clusters based on the density of points rather than assuming a specific shape or size for the clusters. It identifies dense regions as clusters and considers points that are within a certain distance (epsilon, ε) and have a minimum number of neighbors (MinPts) as core points.\n",
    "\n",
    "Differentiating Density Levels: DBSCAN can identify clusters of varying densities because it doesn't require all clusters to have the same density. It distinguishes between core points, border points, and noise points based on their proximity to other points within the dataset.\n",
    "\n",
    "Variable Epsilon Parameter: The epsilon parameter (ε) in DBSCAN allows for flexibility in defining the neighborhood of a point. By adjusting ε, DBSCAN can capture clusters with different density levels. For denser regions, a smaller ε can be used, while for sparser regions, a larger ε may be more appropriate.\n",
    "\n",
    "Connectivity: DBSCAN builds clusters by connecting core points to their neighboring core points. This allows it to form clusters of varying shapes and sizes, accommodating regions with different levels of density within the dataset.\n",
    "\n",
    "Handling Outliers: In regions of low density, points may not meet the criteria to be considered core points or border points. These points are labeled as noise points (outliers) and are not assigned to any cluster, allowing DBSCAN to effectively handle clusters with varying densities without forcing all points into clusters.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. What are some common evaluation metrics used to assess the quality of DBSCAN clustering results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Silhouette Score: The silhouette score measures the cohesion and separation of clusters. It quantifies how similar a point is to its own cluster (cohesion) compared to other clusters (separation). The silhouette score ranges from -1 to 1, where a higher score indicates better clustering. Negative values suggest that data points may have been assigned to the wrong cluster.\n",
    "\n",
    "Davies-Bouldin Index: This index measures the average similarity between each cluster and its most similar cluster, relative to the cluster's internal dissimilarity. A lower Davies-Bouldin index indicates better clustering, with values closer to zero representing better separation between clusters.\n",
    "\n",
    "Dunn Index: The Dunn index evaluates the compactness and separation of clusters. It measures the ratio of the minimum inter-cluster distance to the maximum intra-cluster distance. Higher Dunn index values indicate better clustering, with a larger separation between clusters and tighter clusters.\n",
    "\n",
    "Adjusted Rand Index (ARI): ARI measures the similarity between the true clustering and the clustering result, correcting for chance agreement. It ranges from -1 to 1, where a score of 1 indicates perfect clustering agreement, 0 indicates clustering that is no better than random, and negative values suggest clustering results worse than random.\n",
    "\n",
    "Adjusted Mutual Information (AMI): Similar to ARI, AMI measures the agreement between the true clustering and the clustering result, but it considers the entropy of both the true and predicted clusters. AMI values range from 0 to 1, where higher values indicate better clustering agreement.\n",
    "\n",
    "Homogeneity, Completeness, and V-measure: These metrics evaluate the homogeneity and completeness of clustering. Homogeneity measures how well each cluster contains only data points that are members of a single class, while completeness measures how well all data points of a given class are assigned to the same cluster. The V-measure is the harmonic mean of homogeneity and completeness.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. Can DBSCAN clustering be used for semi-supervised learning tasks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DBSCAN clustering is primarily an unsupervised learning algorithm, meaning it doesn't require labeled data for training. However, it can be utilized in semi-supervised learning tasks in conjunction with labeled data to improve clustering performance or to assist in the labeling process. Here are a few ways DBSCAN clustering can be incorporated into semi-supervised learning:\n",
    "\n",
    "Seed Initialization: In semi-supervised learning, you may have a small amount of labeled data and a larger amount of unlabeled data. You can use DBSCAN to cluster the unlabeled data and then assign labels to the clusters based on the majority label of the labeled data points within each cluster. This can serve as a form of seed initialization for semi-supervised learning algorithms.\n",
    "\n",
    "Cluster Refinement: After initial clustering with DBSCAN, you can use the labeled data to refine the clusters. For example, you could adjust the cluster boundaries based on the labeled data points or reassign points to different clusters if they are inconsistent with the majority label in their cluster.\n",
    "\n",
    "Pseudo-labeling: DBSCAN can be used to generate pseudo-labels for the unlabeled data points based on their cluster assignments. These pseudo-labels can then be used as targets for semi-supervised learning algorithms. However, it's essential to be cautious with pseudo-labeling, as misclustering or noisy data can lead to incorrect labels and degrade performance.\n",
    "\n",
    "Active Learning: In active learning settings, DBSCAN can be used to identify uncertain or ambiguous regions in the data, where additional labeled samples could be most informative. By focusing labeling efforts on these regions, you can iteratively improve the performance of the semi-supervised learning model.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. How does DBSCAN clustering handle datasets with noise or missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) has some inherent mechanisms that allow it to handle datasets with noise or missing values to some extent. Here's how it can handle such datasets:\n",
    "\n",
    "Noise Handling:\n",
    "\n",
    "DBSCAN is designed to handle noise in the dataset. It achieves this by classifying points that are not part of any dense region as noise points (outliers). These noise points are not assigned to any cluster, allowing DBSCAN to effectively identify and ignore outliers in the dataset.\n",
    "When the dataset contains noisy points, DBSCAN will classify them as noise and focus on identifying clusters formed by the dense regions of the data.\n",
    "Handling Missing Values:\n",
    "\n",
    "DBSCAN can handle datasets with missing values by either ignoring the missing values or imputing them. However, missing values can affect the density calculations in DBSCAN.\n",
    "Ignoring missing values: DBSCAN can simply ignore data points with missing values during the clustering process. However, this approach may lead to loss of information, especially if the missing values are prevalent in the dataset.\n",
    "Imputation: Alternatively, missing values can be imputed using various techniques before applying DBSCAN. Imputation methods such as mean imputation, median imputation, or more advanced techniques like k-nearest neighbors (KNN) imputation can be used to fill in missing values.\n",
    "It's important to note that the choice of imputation method can impact the clustering results, and it may introduce biases or distortions into the data.\n",
    "Handling Noisy Features:\n",
    "\n",
    "If the dataset contains features that are noisy or irrelevant to the clustering task, DBSCAN's performance may be affected. In such cases, it's advisable to preprocess the data by removing or reducing the influence of noisy features before applying DBSCAN.\n",
    "Feature selection techniques or dimensionality reduction methods like PCA (Principal Component Analysis) can help in identifying and removing noisy features, thereby improving the clustering results.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q11. Implement the DBSCAN algorithm using a python programming language, and apply it to a sample\n",
    "dataset. Discuss the clustering results and interpret the meaning of the obtained clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster labels: [ 1.  1.  1.  2.  2. -1.]\n",
      "Noise points:\n",
      "[[25 80]]\n",
      "Cluster 1.0 :\n",
      "[[1 2]\n",
      " [2 2]\n",
      " [2 3]]\n",
      "Cluster 2.0 :\n",
      "[[8 7]\n",
      " [8 8]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "class DBSCAN:\n",
    "    def __init__(self, eps=0.5, min_samples=5):\n",
    "        self.eps = eps\n",
    "        self.min_samples = min_samples\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X = X\n",
    "        self.labels = np.zeros(len(X))  # Initialize labels\n",
    "        self.cluster_id = 0\n",
    "\n",
    "        for i in range(len(X)):\n",
    "            if self.labels[i] != 0:\n",
    "                continue\n",
    "            neighbors = self._region_query(i)\n",
    "            if len(neighbors) < self.min_samples:\n",
    "                self.labels[i] = -1  # Mark as noise\n",
    "            else:\n",
    "                self.cluster_id += 1\n",
    "                self._expand_cluster(i, neighbors, self.cluster_id)\n",
    "\n",
    "    def _region_query(self, idx):\n",
    "        return np.where(pairwise_distances([self.X[idx]], self.X) <= self.eps)[1]\n",
    "\n",
    "    def _expand_cluster(self, idx, neighbors, cluster_id):\n",
    "        self.labels[idx] = cluster_id\n",
    "        i = 0\n",
    "        while i < len(neighbors):\n",
    "            neighbor = neighbors[i]\n",
    "            if self.labels[neighbor] == -1:  # Noise points\n",
    "                self.labels[neighbor] = cluster_id\n",
    "            elif self.labels[neighbor] == 0:  # Unclassified\n",
    "                self.labels[neighbor] = cluster_id\n",
    "                new_neighbors = self._region_query(neighbor)\n",
    "                if len(new_neighbors) >= self.min_samples:\n",
    "                    neighbors = np.concatenate((neighbors, new_neighbors))\n",
    "            i += 1\n",
    "\n",
    "# Sample dataset\n",
    "X = np.array([[1, 2], [2, 2], [2, 3],\n",
    "              [8, 7], [8, 8], [25, 80]])\n",
    "\n",
    "# Applying DBSCAN\n",
    "dbscan = DBSCAN(eps=3, min_samples=2)\n",
    "dbscan.fit(X)\n",
    "\n",
    "# Print cluster labels\n",
    "print(\"Cluster labels:\", dbscan.labels)\n",
    "\n",
    "# Interpretation of clusters\n",
    "unique_labels = np.unique(dbscan.labels)\n",
    "for label in unique_labels:\n",
    "    if label == -1:\n",
    "        print(\"Noise points:\")\n",
    "        print(X[dbscan.labels == label])\n",
    "    else:\n",
    "        print(\"Cluster\", label, \":\")\n",
    "        print(X[dbscan.labels == label])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
